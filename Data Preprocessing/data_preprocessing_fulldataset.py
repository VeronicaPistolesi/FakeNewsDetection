# -*- coding: utf-8 -*-
"""data_preprocessing_fulldataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19P2sMVAn5PfmBcl3QIQqSskQyMnFQCPQ
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Libraries"""

import pandas as pd
import numpy as np
import nltk
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## Data Loading"""

train = pd.read_csv('/content/drive/Shareddrives/ProgettoHLT/FakeNewsDetection/Data/train.csv')
test = pd.read_csv('/content/drive/Shareddrives/ProgettoHLT/FakeNewsDetection/Data/test.csv')

"""## Brief Data Analysis"""

words = [] # Number of total words
for article in train.text:
    for word in article.split():
        words.append(word)
       
unique_words = len(list(set(words))) # Number of unique words 
print('Number of unique words: ', unique_words)

nltk.download('punkt')

articles = [text for text in train.text]
articles_len = []

for article in articles:
    articles_len.append(len(nltk.word_tokenize(article)))
    
print('Number of news:', len(articles)) # Number of news

median = round(np.median(articles_len)) # Median 
print('Median length of the news:', median)

# Adding length column
train['length']= articles_len

"""### Training Set Distribution"""

# Training Set Distribution
sns.countplot(y="label", palette="coolwarm", data=train).set_title('True and Fake News Distribution (training set)')
plt.show()

# Number of True News in the Training Set  
print('Number of True News in the Training Set:', len(train.loc[train['label']==0]))

# Number of Fake News in the Training Set
print('Number of Fake News in the Training Set:', len(train.loc[train['label']==1]))

"""## Tokenization and Padding"""

# Training Set
X_train = train.text
Y_train = train.label

# Test Set
X_test = test.text
Y_test = test.label

# Sequences of tokenized words
tokenizer = Tokenizer(num_words= unique_words)
tokenizer.fit_on_texts(X_train)
train_sequences = tokenizer.texts_to_sequences(X_train)
test_sequences = tokenizer.texts_to_sequences(X_test)
word_index = tokenizer.word_index

# Padding
padded_train = pad_sequences(train_sequences, maxlen = median, padding = 'post', truncating = 'post')
padded_test = pad_sequences(test_sequences, maxlen = median, padding= 'post', truncating = 'post')